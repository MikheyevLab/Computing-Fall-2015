---
layout: post
title: 10. High performance computing
categories: []
tags:
  - news
published: true
---
# High performance computing on OIST infrastructure
Science is more and more moving towards problem that involve data sizes that are only solvable with HPC. HPC stands for computing that occur on networked cluster of machines or on so called supercomputers. OIST currently operates two cluster Sango and Tombo, where the latter one is going to be phased out.

## Concepts
A cluster consists of several independent computers (called nodes) that are connected over a very fast network. Each node contains several CPUs (processors) that are clustered in NUMAs. Each cluster of CPUs share memory, but they also can access memory in othe NUMA groups. Communication between CPUs in a node is fast and communication between nodes is relatively slow, in a setting where computation is distributed across several nodes, communication can become a bottleneck.

There are two main paradigms of parallel computation, shared-memory and message passing. In the shared-memory paradigm every process has access to the same memory and can read and write to it at any point in time, access to critical regions has to be regulated by programming constructs like mutexes. The message passing model is based around the idea that each process has its own memory and necessary information are passed around via message and synchronizations of the computation happens only during the communication step. Shared memory programs are most of the time easier to write, but they can only run on *one* node, since the basic assumption is that memory access is cheap. Reasoning about message-passing programs can be harder, but computations can span thousands of nodes.

Sidestepping that issue are embarrassingly parallel problems, where no communication is necessary between processes. As an example performing 10000 stochastic simulation at once.

Lastly in modern supercomputers accelerator cards are used. An accelerator card is either a GPU or a specialized hardware like the Intel Xeon Phi. The general idea is that combining a hundreds of relative simple compute units, that all perform the same operation in parallel over a dataset. These accelerators are particular suited for vectorizable problems in numerical simulations.

Most cluster are controlled by a scheduler system that manages jobs submissions and tries to optimally distribute workload over the cluster and terminates jobs that run longer than there allocated time.

## SLURM
The scheduler used at OIST is called SLURM. For submitting jobs you can either use the commandlie tools directly or write scripts that do that for you.

### List of useful commands

- Job submission
  - `salloc` Allocate resources and create shell
  - `sbatch` Submit a job script for later execution
  - `srun` Submit job for execution (standalone or as part of `salloc`)
- Job info
  - `sacct` Accounting information for jobs
  - `sstat` Statistics about jobs
  - `squeue` Reports status of jobs
  - `sinfo` State of partitions and nodes
- `scancel` Cancel Jobs

### Array Jobs
http://slurm.schedmd.com/job_array.html

### Interactive Sessions
In order to run graphical programs on Sango you have to connect to Sango with `ssh -Y`

Example of a session would look like

```bash
ssh -Y sango
module load matlab
salloc -c 4 --mem=8g # Allocate 4 Cores and 8Gb of memory
srun --x11=last --pty matlab
```

### Modules

## Resources
- SCS @ OIST https://groups.oist.jp/scs
  SCS is the normative information source about HPC @ OIST. They manage and maintain our clusters and provide support for research.
  - https://groups.oist.jp/scs/getting-started#gs_prac
- SLURM http://slurm.schedmd.com/
  - Documentation
  - Tutorial

## Mounting networked drives
You can directly interact with the Sango filesystem from your own computer. This can be handy for running your favorite text editor and modifying source code that lives on Sango.

# Homework
The homework is due on *November 27 2015* at *13:00pm* (eg. noon).

1. Take a fairly large file of your choice (a few gb) and copy it to Sango in two ways. First, use `scp`. Next, mount the `/work` folder and copy it using the terminal using `cp`. Which way is faster?
2. Create a directory with an arbitrary number of files (at least 10), each containing a number. Now write a SLURM script that performs an operation on each one of these files in parallel, printing the number inside. Make sure your script generalizes to any number of files.
3. 
